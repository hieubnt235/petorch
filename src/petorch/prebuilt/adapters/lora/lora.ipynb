{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:40:13.057335Z",
     "start_time": "2025-08-05T13:40:11.086083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from petorch.prebuilt.lora import LoraLinear\n",
    "from torch import nn\n",
    "import torch\n",
    "from petorch.adapter import AdapterAPI, BaseModelAdaptionConfig, BaseAdapter\n",
    "from pydantic import PositiveInt, NonNegativeFloat\n",
    "from typing import cast"
   ],
   "id": "df472e9794acddd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:40:13.080436Z",
     "start_time": "2025-08-05T13:40:13.067335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dummy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = (8,8)\n",
    "        self.conv = nn.Conv2d(3,3,3,padding='same')\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8*8*3, 256)\n",
    "        self.fc2 = nn.Linear(256, 100)\n",
    "        self.fc3 = nn.Linear(100, 16)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        assert input.shape[2:] == self.input_shape, f\"{input.shape[:2]}-{self.input_shape}\"\n",
    "        x = self.conv(input)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LoraLinearConfig(BaseModelAdaptionConfig):\n",
    "    rank: PositiveInt = 8\n",
    "    alpha: PositiveInt = 16\n",
    "    dropout: NonNegativeFloat = 0.1\n",
    "\n",
    "\n",
    "    def dispatch_adapter(self, fpname: str, base_layer: nn.Module, *args, **kwargs)-> BaseAdapter | None:\n",
    "        if isinstance(base_layer,nn.Linear):\n",
    "            return LoraLinear(cast(nn.Linear,base_layer),self)\n",
    "model = Dummy()\n",
    "org_model = deepcopy(model)\n",
    "config = LoraLinearConfig()\n",
    "sample = torch.rand([2,3,8,8])\n",
    "output = org_model(sample)\n",
    "org_model"
   ],
   "id": "d20bb8d745e15e72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dummy(\n",
       "  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=192, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:40:13.134356Z",
     "start_time": "2025-08-05T13:40:13.123849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(AdapterAPI.add_adapter(model, config))\n",
    "model"
   ],
   "id": "85f307f6a6bf2973",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fc1', 'fc2', 'fc3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dummy(\n",
       "  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): AdaptedLayer(\n",
       "    (base_layer): Linear(in_features=192, out_features=256, bias=True)\n",
       "    (active_adapters): ModuleDict()\n",
       "    (non_active_adapters): ModuleDict(\n",
       "      (default): LoraLinear(\n",
       "        (lora_A): Linear(in_features=192, out_features=8, bias=True)\n",
       "        (lora_B): Linear(in_features=8, out_features=256, bias=True)\n",
       "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc2): AdaptedLayer(\n",
       "    (base_layer): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (active_adapters): ModuleDict()\n",
       "    (non_active_adapters): ModuleDict(\n",
       "      (default): LoraLinear(\n",
       "        (lora_A): Linear(in_features=256, out_features=8, bias=True)\n",
       "        (lora_B): Linear(in_features=8, out_features=100, bias=True)\n",
       "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc3): AdaptedLayer(\n",
       "    (base_layer): Linear(in_features=100, out_features=16, bias=True)\n",
       "    (active_adapters): ModuleDict()\n",
       "    (non_active_adapters): ModuleDict(\n",
       "      (default): LoraLinear(\n",
       "        (lora_A): Linear(in_features=100, out_features=8, bias=True)\n",
       "        (lora_B): Linear(in_features=8, out_features=16, bias=True)\n",
       "        (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:41:04.682971Z",
     "start_time": "2025-08-05T13:41:04.674090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output2 = model(sample)\n",
    "assert torch.all(output==output2)"
   ],
   "id": "d208dc9f199e59b4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:41:07.209912Z",
     "start_time": "2025-08-05T13:41:07.201361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    AdapterAPI.activate_adapter(model, 'abc')\n",
    "except ValueError as e:\n",
    "    print(e)"
   ],
   "id": "67ba89f9efb5437a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model does not have adapter named `abc`.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:41:10.842337Z",
     "start_time": "2025-08-05T13:41:10.829827Z"
    }
   },
   "cell_type": "code",
   "source": "AdapterAPI.activate_adapter(model)",
   "id": "5710442b8c464642",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:41:28.547071Z",
     "start_time": "2025-08-05T13:41:28.542343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output3 = model(sample)\n",
    "assert not torch.all(output==output3)"
   ],
   "id": "1dcbb41e2b4d726f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:18:06.821443Z",
     "start_time": "2025-08-05T13:18:06.813150Z"
    }
   },
   "cell_type": "code",
   "source": "AdapterAPI.remove_adapter(model, \"default\")",
   "id": "b90352490368cbae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:18:07.855964Z",
     "start_time": "2025-08-05T13:18:07.840227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output4 = model(sample)\n",
    "torch.all(output==output4)"
   ],
   "id": "57adf860b4d74d4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T13:18:10.042704Z",
     "start_time": "2025-08-05T13:18:09.778997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from typing import cast\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "from pydantic import PositiveInt, NonNegativeFloat, BaseModel\n",
    "from torch import nn\n",
    "\n",
    "from petorch.adapter import BaseModelAdaptionConfig, BaseAdapter, AdapterAPI\n",
    "\n",
    "\n",
    "class DummyAdapter(BaseAdapter):\n",
    "    \"\"\"\n",
    "    Dummy LinearLora\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer: nn.Linear, config: \"BaseModelAdaptionConfig\"):\n",
    "        assert isinstance(\n",
    "            base_layer, nn.Linear\n",
    "        ), f\"Base layer must has type {nn.Linear}, got {type(base_layer)}.\"\n",
    "        super().__init__(base_layer, config)\n",
    "\n",
    "        self.lora_A = nn.Linear(base_layer.in_features, self.rank)\n",
    "        self.lora_B = nn.Linear(self.rank, base_layer.out_features)\n",
    "        self.lora_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.scale = getattr(self.config, \"scale\", None) or 1\n",
    "\n",
    "    @property\n",
    "    def rank(self) -> int:\n",
    "        return self.config.rank\n",
    "\n",
    "    @property\n",
    "    def alpha(self) -> float:\n",
    "        return self.config.alpha\n",
    "\n",
    "    @property\n",
    "    def dropout(self) -> float:\n",
    "        return self.config.dropout\n",
    "\n",
    "    @property\n",
    "    def scaling(self) -> float:\n",
    "        return self.scale * self.alpha / self.rank\n",
    "\n",
    "    @classmethod\n",
    "    def pre_validate_config(cls, config: \"BaseModelAdaptionConfig\") -> None:\n",
    "        class ConfigValidator(BaseModel):\n",
    "            rank: PositiveInt\n",
    "            alpha: PositiveInt\n",
    "            dropout: NonNegativeFloat\n",
    "            adapter_name: str\n",
    "\n",
    "        ConfigValidator.model_validate(config, from_attributes=True)\n",
    "\n",
    "    def forward(self, batch_input: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        output = self.base_layer(batch_input)\n",
    "        return (\n",
    "            output\n",
    "            + self.lora_B(self.lora_A(self.lora_dropout(batch_input))) * self.scaling\n",
    "        )\n",
    "\n",
    "class DummySubModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 100)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        x = self.fc1(input)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Dummy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = (8, 8)\n",
    "        self.conv = nn.Conv2d(3, 3, 3, padding=\"same\")\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sub_model = DummySubModel()\n",
    "        self.fc = nn.Linear(100, 16)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        assert input.shape[2:] == self.input_shape,  f\"{input.shape[:2]}-{self.input_shape}\"\n",
    "        x = self.conv(input)\n",
    "        x = self.flatten(x)\n",
    "        x = self.sub_model(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyConfig(BaseModelAdaptionConfig):\n",
    "    rank: PositiveInt = 8\n",
    "    alpha: PositiveInt = 16\n",
    "    dropout: NonNegativeFloat = 0.1\n",
    "\n",
    "    def dispatch_adapter(\n",
    "        self, fpname: str, base_layer: nn.Module, *args, **kwargs\n",
    "    ) -> BaseAdapter | None:\n",
    "        if isinstance(base_layer, nn.Linear):\n",
    "            return DummyAdapter(cast(nn.Linear, base_layer), self)\n",
    "\n",
    "\n",
    "def test_api():\n",
    "    adapter_name = \"test_adapter\"\n",
    "    model = Dummy()\n",
    "    org_model = deepcopy(model)\n",
    "    config = DummyConfig(adapter_name=adapter_name)\n",
    "    sample = torch.rand([2, 3, 8, 8])\n",
    "\n",
    "    output1 = org_model(sample)\n",
    "\n",
    "    # Test add_adapter\n",
    "    target_fqn = [\n",
    "        \"fc\",\n",
    "        \"sub_model.fc1\",\n",
    "        \"sub_model.fc2\"\n",
    "    ]\n",
    "    target_fqn.sort()\n",
    "    fqn = AdapterAPI.add_adapter(model, config)\n",
    "    fqn.sort()\n",
    "    assert fqn == target_fqn\n",
    "\n",
    "test_api()\n"
   ],
   "id": "903a45fd539638c6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:44:59.392350Z",
     "start_time": "2025-08-05T15:44:44.108530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Qwen2ForCausalLM, TorchAoConfig\n",
    "from torchao.dtypes import NF4Tensor, to_nf4\n",
    "from torchao.quantization import register_quantize_module_handler, Float8WeightOnlyConfig, ModuleFqnToConfig\n",
    "from dataclasses import dataclass\n",
    "from torchao.core.config import AOBaseConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "import types\n",
    "from torchao.utils import get_model_size_in_bytes\n",
    "@dataclass\n",
    "class NF4Config(AOBaseConfig):\n",
    "    block_size: int = 64\n",
    "    scaler_block_size: int = 256\n",
    "\n",
    "def linear_module_repr(module: nn.Linear):\n",
    "    return f\"in_features={module.weight.shape[1]}, out_features={module.weight.shape[0]}, weight={module.weight}, dtype={module.weight.dtype}\"\n",
    "\n",
    "@register_quantize_module_handler(NF4Config)\n",
    "def _nf4_weight_only_transform(\n",
    "    module: torch.nn.Module,\n",
    "    config: NF4Config,\n",
    ") -> torch.nn.Module:\n",
    "    new_weight = to_nf4(module.weight, config.block_size, config.scaler_block_size)\n",
    "    module.weight = nn.Parameter(new_weight, requires_grad=False) # Freeze\n",
    "    module.extra_repr = types.MethodType(\n",
    "        linear_module_repr,\n",
    "        module\n",
    "    )\n",
    "    return module\n",
    "\n",
    "config = TorchAoConfig(NF4Config())\n",
    "\n",
    "model = quantized_model = Qwen2ForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    ")\n",
    "\n",
    "quantized_model = Qwen2ForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    quantization_config = config\n",
    ")\n",
    "model_size = get_model_size_in_bytes(model)\n",
    "quantized_model_size = get_model_size_in_bytes(quantized_model)\n",
    "print(model_size) # 2520669824\n",
    "print(quantized_model_size) # 1273966688\n",
    "print(quantized_model_size/model_size) # 0.5054079974577425"
   ],
   "id": "8482c5c4f07665be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520669824\n",
      "1273966688\n",
      "0.5054079974577425\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare peft conv and einsum",
   "id": "139e1d93089af975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "kernel_size = stride  = (1,)*2\n",
    "c = 512\n",
    "\n",
    "base = nn.Conv2d(c,int(c/2),3,1,'same',bias=False)\n",
    "\n",
    "lora_A = nn.Conv2d(c,8,3,1,'same', bias = False)\n",
    "kernel_size = stride  = (1,)*2\n",
    "lora_B = nn.Conv2d(8,int(c/2),kernel_size,stride, bias = False)\n",
    "\n",
    "print(base_w:=base.weight.numel())\n",
    "print(lora_w:=(lora_A.weight.numel()+lora_B.weight.numel()))\n",
    "print(lora_w/base_w)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "fw, einw = None, None\n",
    "start = time.time()\n",
    "for i in range(5000):\n",
    "    fw = nn.functional.conv2d(lora_A.weight.transpose(0, 1), lora_B.weight).transpose(0,1)\n",
    "print(\"time for peft method: \",time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    einw = torch.einsum(\"o r ..., r i ... -> o i ...\", lora_B.weight, lora_A.weight)\n",
    "\n",
    "print(\"time for einsum method: \", time.time() - start)\n",
    "print(torch.allclose(fw,einw))\n",
    "print(fw.shape== base.weight.shape == einw.shape)"
   ],
   "id": "88c1c261fe628599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample = torch.randn([2,c,256,256])\n",
    "output = base(sample) + lora_B(lora_A(sample))\n",
    "output_fw = nn.functional.conv2d(sample, base.weight + fw, padding='same')\n",
    "output_einw = nn.functional.conv2d(sample, base.weight+einw, padding='same')\n",
    "\n",
    "print(torch.allclose(output, output_fw, atol=1e-5)) # False, max abs\n",
    "print(torch.allclose(output, output_einw, atol=1e-5)) # True\n",
    "print(torch.allclose(output_einw, output_fw, atol=1e-5))\n",
    "\n",
    "print(abs((output - output_fw)).max()) # 3.0\n",
    "print(abs(output - output_einw).max()) # 1.3e-5\n",
    "print(abs(output_fw - output_einw).max()) # 3.0"
   ],
   "id": "d96a8e827f90ed32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85567dd1d7ebbf12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
