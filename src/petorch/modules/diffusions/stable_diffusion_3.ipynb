{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Components (Migrate from Stable Diffusion)**\n",
    "Quick migration mental model (old → new)\n",
    "\n",
    "1. U-Net → Transformer (MMDiT-X)\n",
    "\n",
    "2. 1× CLIP → 2× CLIP + 1× T5-XXL (FROZEN ALL, T5 can be obmit when inference)\n",
    "T5 for language understanding, CLIP for embedding.\"Multi-encoder fusion providing both detailed and global context, plus advanced linguistic understanding with T5; supports optional omission to manage VRAM.\"\n",
    "\n",
    "3. Same VAE role (encode/decode latents)\n",
    "\n",
    "4. DDIM/DPM schedulers → FlowMatch-Euler/Heun (+ shift)\n",
    "\n",
    "5. CFG only → CFG + optional SLG\n"
   ],
   "id": "8951867ab16e162f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Module                            | SD2                                                   | SD3                                                                    | Notes                                         |\n",
    "| --------------------------------- | ----------------------------------------------------- | ---------------------------------------------------------------------- | --------------------------------------------- |\n",
    "| **Text Encoders**                 | OpenCLIP-ViT-H/14                                     | CLIP-ViT/L, OpenCLIP-ViT/G (**new**), T5-XXL (**new**)                 | Multi-encoder fusion is the biggest change.   |\n",
    "| **Tokenizer**                     | CLIP tokenizer                                        | CLIP tokenizer, T5 tokenizer (**new**)                                 | To handle multiple encoder types.             |\n",
    "| **UNet / Denoiser**               | UNet2DConditionModel (ResNet + CrossAttention blocks) | **MM-DiT (Multimodal Diffusion Transformer)** (**new**)                | Transformer backbone replaces CNN-heavy UNet. |\n",
    "| **Noise Scheduler**               | DDIM, PNDM, Euler, etc.                               | Flow Matching Scheduler (**new**, but also supports DDIM etc.)         | Flow matching improves training stability.    |\n",
    "| **Variational Autoencoder (VAE)** | VAE-based latent compression (256→64)                 | Same (slightly improved VAE)                                           | Still compresses images into latent space.    |\n",
    "| **Conditioning Mechanism**        | Cross-attention with CLIP embeddings                  | Cross-attention + pooled embeddings + T5 embeddings (**new features**) | Richer conditioning signals.                  |\n",
    "| **Safety Checker**                | Optional NSFW detector                                | Similar, integrated                                                    | No big change.                                |\n",
    "| **Optimizer**                     | AdamW, EMA                                            | Same, but scaled for DiT training                                      | Training is heavier due to larger encoders.   |\n"
   ],
   "id": "d615e09f26370de8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pseudocode for Stable Diffusion 3 Inference\n",
    "\n",
    "def sd3_inference(prompt, num_steps=50, guidance_scale=7.5):\n",
    "    # INPUT: text prompt (string)\n",
    "    # OUTPUT: generated image (RGB)\n",
    "\n",
    "    # [NEW in SD3] Multiple text encoders\n",
    "    clip_l_emb = CLIP_ViT_L.encode(prompt)          # encoder 1\n",
    "    clip_g_emb = OpenCLIP_ViT_G.encode(prompt)      # encoder 2\n",
    "    t5_emb      = T5_XXL.encode(prompt)             # encoder 3\n",
    "    conditioning = fuse_embeddings([clip_l_emb, clip_g_emb, t5_emb])\n",
    "\n",
    "    # Latent init (same as SD2)\n",
    "    latents = sample_gaussian_noise(shape=(latent_channels, H//8, W//8))\n",
    "\n",
    "    # Iterative denoising\n",
    "    for t in scheduler.timesteps(num_steps):\n",
    "        # [NEW] MM-DiT backbone with flow matching scheduler\n",
    "        velocity = MM_DiT(latents, conditioning, t)  # predict velocity/flow\n",
    "        latents = scheduler.step(velocity, t, latents, guidance_scale)\n",
    "\n",
    "    # Decode to image (same as SD2)\n",
    "    image = VAE.decode(latents)\n",
    "    return image\n"
   ],
   "id": "97cbed8af5eee315"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9fe78a2a59d8aa52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pseudocode for Stable Diffusion 3 Training\n",
    "for batch in dataloader:\n",
    "    images, captions = batch\n",
    "    # INPUT: paired (image, text)\n",
    "    # OUTPUT: trained weights for MM-DiT\n",
    "\n",
    "    # Latent encoding (same as SD2)\n",
    "    latents = VAE.encode(images)\n",
    "\n",
    "    # Sample timestep + noise (same as SD2)\n",
    "    noise = sample_gaussian_noise_like(latents)\n",
    "    t = sample_random_timestep()\n",
    "    noisy_latents = scheduler.add_noise(latents, noise, t)\n",
    "\n",
    "    # [NEW in SD3] Multi-encoder conditioning\n",
    "    clip_l_emb = CLIP_ViT_L.encode(captions)\n",
    "    clip_g_emb = OpenCLIP_ViT_G.encode(captions)\n",
    "    t5_emb     = T5_XXL.encode(captions)\n",
    "    conditioning = fuse_embeddings([clip_l_emb, clip_g_emb, t5_emb])\n",
    "\n",
    "    # [NEW] MM-DiT predicts velocity instead of noise\n",
    "    velocity_pred = MM_DiT(noisy_latents, conditioning, t)\n",
    "\n",
    "    # [NEW] Flow-matching loss\n",
    "    loss = mse_loss(velocity_pred, true_velocity(latents, noise, t))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ],
   "id": "72155b927991a9de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:25:31.324618Z",
     "start_time": "2025-08-29T09:25:31.317583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" SD2\n",
    "Text Prompt ──> OpenCLIP-ViT-H/14 ──> Text Embeddings\n",
    "                                         │\n",
    "                                         ▼\n",
    "   Latent Noise ──> UNet (ResNet+Attention) ──> ε-prediction (noise)\n",
    "                                         │\n",
    "                              Scheduler (DDPM/DDIM/PNDM)\n",
    "                                         │\n",
    "                                         ▼\n",
    "                                 Latent Image\n",
    "                                         │\n",
    "                                         ▼\n",
    "                                VAE Decoder → RGB Image\n",
    "\n",
    "SD3\n",
    "Text Prompt ──> [CLIP-ViT/L]  ─┐\n",
    "              [OpenCLIP-ViT/G] ├──> Fused Text Embeddings ──┐\n",
    "              [T5-XXL]        ─┘                            │  [NEW]\n",
    "                                                           ▼\n",
    "   Latent Noise ──> MM-DiT Transformer ──> Velocity (dx/dt prediction) [NEW]\n",
    "                                                             │\n",
    "                                           Scheduler (Flow Matching) [NEW]\n",
    "                                                             │\n",
    "                                                             ▼\n",
    "                                                     Latent Image\n",
    "                                                             │\n",
    "                                                             ▼\n",
    "                                                    VAE Decoder → RGB Image\n",
    "\n",
    "\"\"\"\n",
    "None"
   ],
   "id": "616082f17c5bce29",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **1. Transformer**",
   "id": "60c677dc6aa2de1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:03:33.086150Z",
     "start_time": "2025-08-29T09:03:25.351633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from diffusers import SD3Transformer2DModel\n",
    "import torch\n",
    "dtype = torch.bfloat16\n",
    "model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
    "\n",
    "transformer = SD3Transformer2DModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype = dtype,\n",
    ")\n",
    "transformer"
   ],
   "id": "87e833c91a65d218",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SD3Transformer2DModel(\n",
       "  (pos_embed): PatchEmbed(\n",
       "    (proj): Conv2d(16, 1536, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (time_text_embed): CombinedTimestepTextProjEmbeddings(\n",
       "    (time_proj): Timesteps()\n",
       "    (timestep_embedder): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=256, out_features=1536, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "    (text_embedder): PixArtAlphaTextProjection(\n",
       "      (linear_1): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "      (act_1): SiLU()\n",
       "      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (context_embedder): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-12): 13 x JointTransformerBlock(\n",
       "      (norm1): SD35AdaLayerNormZeroX(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1536, out_features=13824, bias=True)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (norm1_context): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (norm_added_q): RMSNorm()\n",
       "        (norm_added_k): RMSNorm()\n",
       "      )\n",
       "      (attn2): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff_context): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13-22): 10 x JointTransformerBlock(\n",
       "      (norm1): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (norm1_context): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (norm_added_q): RMSNorm()\n",
       "        (norm_added_k): RMSNorm()\n",
       "      )\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff_context): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): JointTransformerBlock(\n",
       "      (norm1): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (norm1_context): AdaLayerNormContinuous(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm_added_q): RMSNorm()\n",
       "        (norm_added_k): RMSNorm()\n",
       "      )\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_out): AdaLayerNormContinuous(\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1536, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:03:36.231761Z",
     "start_time": "2025-08-29T09:03:35.604231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from petorch import AdapterAPI\n",
    "from petorch.prebuilt.configs import LoraConfig\n",
    "from petorch.utilities.func import get_module_num_parameters, freeze_module\n",
    "config = LoraConfig(adapter_name='default', rank=8, alpha=16)\n",
    "def freeze_adapter_and_print_info(module:torch.nn.Module):\n",
    "    frozen_params = freeze_module(module)\n",
    "    AdapterAPI.add_adapter(module,config, activate=False)\n",
    "    train_params, non_train_params = get_module_num_parameters(module)\n",
    "\n",
    "    assert non_train_params == frozen_params, f\"{non_train_params} != {frozen_params}\"\n",
    "    print(f\"Train params:{train_params:,}, Non train params: {non_train_params:,}\")\n",
    "    print(train_params / non_train_params)\n",
    "freeze_adapter_and_print_info(transformer)"
   ],
   "id": "cf8ed0a832511d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m\u001B[1m[2025-08-29 09:03:35.841]\u001B[0m\u001B[32m\u001B[0m \u001B[1m[PID 33309 | INFO    ]\u001B[0m \u001B[36m\u001B[3mpetorch.utilities.logger:setup_logger:110 :~ \u001B[0m\u001B[36m\u001B[0m \u001B[1mLog level is set to `INFO`.\u001B[0m\n",
      "Train params:16,505,856, Non train params: 2,243,171,520\n",
      "0.007358267458745196\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "from diffusers import StableDiffusion3Pipeline, StableDiffusion3Img2ImgPipeline"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\"A red bus driving on a bridge\"\n",
    "| Encoder | Token embeddings (shape)                             | Meaning                           | Pooled embedding (shape) |\n",
    "| ------- | ---------------------------------------------------- | --------------------------------- | ------------------------ |\n",
    "| CLIP-L  | `(77, 768)`                                          | Each of 77 tokens → 768-d vectors | `(768,)`                 |\n",
    "| CLIP-G  | `(77, 1024)`                                         | Same 77 tokens → 1024-d vectors   | `(1024,)`                |\n",
    "| T5-XXL  | `(32, 4096)` (assuming 32 tokens from SentencePiece) | Contextualized embeddings         | —                        |\n",
    "\n",
    "After projection:\n",
    "\n",
    "CLIP-L → (77, 4096)\n",
    "\n",
    "CLIP-G → (77, 4096)\n",
    "\n",
    "T5-XXL → (32, 4096)\n",
    "\n",
    "These sequences can be concatenated → (186, 4096) (77+77+32 tokens).\n",
    "That’s the fused text embedding sequence given to MM-DiT."
   ],
   "id": "fae20619eba0cfa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Term                                          | What it means                                                      | Example output                                                              |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------ | --------------------------------------------------------------------------- |\n",
    "| **Token embedding (lookup layer)**            | Maps each token ID → vector (no context)                           | `(seq_len, hidden_dim)` but same word in different contexts has same vector |\n",
    "| **Contextualized embedding (encoder output)** | Each token vector **incorporates context from surrounding tokens** | `(seq_len, hidden_dim)` but now “red” in “red bus” ≠ “red apple”            |\n",
    "| **Sequence embedding / sentence embedding**   | Compresses **whole sequence** into **single vector**               | `(hidden_dim,)` or `(1, hidden_dim)` — usually via pooling or \\[CLS] token  |\n"
   ],
   "id": "45bdf362b1a18e4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://huggingface.co/blog/sd3",
   "id": "58619857182e06cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "63bac939cf43a9d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
